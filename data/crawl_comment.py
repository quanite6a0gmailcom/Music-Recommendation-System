from youtube_search import YoutubeSearch
from youtube_comment_downloader import YoutubeCommentDownloader, SORT_BY_POPULAR
import pandas as pd
import re
import time
import os
import csv

# --- C·∫§U H√åNH ---
INPUT_FILE = r'spotify_songs.csv/spotify_songs_final.csv'
OUTPUT_FILE = r"spotify_songs.csv/spotify_songs_final_comments_merged.csv"
MAX_COMMENTS = 50  # S·ªë l∆∞·ª£ng comment mu·ªën l·∫•y m·ªói b√†i ƒë·ªÉ g·ªôp

#Read input file
df = pd.read_csv(INPUT_FILE,encoding='utf-8-sig')
total_songs = len(df)
df['comment'] = ""

#Read output file
start_index = 0
file_exists = os.path.isfile(OUTPUT_FILE)

if file_exists:
    # ƒê·∫øm s·ªë d√≤ng ƒë√£ c√≥ trong file output ƒë·ªÉ bi·∫øt c·∫ßn ch·∫°y ti·∫øp t·ª´ ƒë√¢u
    with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
        # Tr·ª´ 1 v√¨ d√≤ng ƒë·∫ßu l√† header
        row_count = sum(1 for row in f) - 1
        start_index = row_count
        print(f"üîÑ Detected old files. The {start_index} files have been run. Continuing...")
else:
    print("üöÄ Start a completely new run....")

def get_video_id(keyword):
    """T√¨m video ID t·ª´ t·ª´ kh√≥a"""
    try:
        results = YoutubeSearch(keyword, max_results=1).to_dict()
        if results:
            return results[0]['id']
    except:
        return None
    return None

def clean_text(text):
    """
    H√†m l√†m s·∫°ch comment:
    1. Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng.
    2. X√≥a Link (http...).
    3. X√≥a k√Ω t·ª± ƒë·∫∑c bi·ªát (icon, emoji...), ch·ªâ gi·ªØ l·∫°i ch·ªØ v√† s·ªë.
    4. X√≥a d·∫•u xu·ªëng d√≤ng (\n).
    """
    if not isinstance(text, str):
        return ""
    
    # 1. Chuy·ªÉn ch·ªØ th∆∞·ªùng
    text = text.lower()
    
    # 2. X√≥a URL/Link
    text = re.sub(r'http\S+', '', text)
    
    # 3. X√≥a c√°c k√Ω t·ª± kh√¥ng ph·∫£i l√† ch·ªØ (gi·ªØ l·∫°i ti·∫øng Vi·ªát v√† s·ªë)
    # \w bao g·ªìm [a-zA-Z0-9_] v√† c√°c k√Ω t·ª± unicode ti·∫øng Vi·ªát
    # N·∫øu mu·ªën gi·ªØ d·∫•u c√¢u (.,?!), h√£y b·ªè d√≤ng n√†y
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # 4. Thay th·∫ø xu·ªëng d√≤ng b·∫±ng kho·∫£ng tr·∫Øng (QUAN TR·ªåNG ƒê·ªÇ TH√ÄNH 1 D√íNG)
    text = text.replace('\n', ' ').replace('\r', ' ')
    
    # 5. X√≥a kho·∫£ng tr·∫Øng th·ª´a (v√≠ d·ª•: "  a   b " -> "a b")
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def process_comments(track_name,track_artist):
    downloader = YoutubeCommentDownloader()
    final_data = []

    search_query = f"{track_name} {track_artist}"
    print(f"üîé ƒêang t√¨m: {search_query}...", end=" ")
    
    video_id = get_video_id(search_query)
    
    if video_id:
        try:
            # L·∫•y comment (Generator)
            generator = downloader.get_comments(video_id, sort_by=SORT_BY_POPULAR)
            
            raw_comments = []
            count = 0
            
            # V√≤ng l·∫∑p l·∫•y t·ª´ng comment
            for comment in generator:
                text = comment['text']
                
                # --- L√ÄM S·∫†CH NGAY L·∫¨P T·ª®C ---
                cleaned_text = clean_text(text)
                
                if cleaned_text: # N·∫øu comment kh√¥ng r·ªóng sau khi clean
                    raw_comments.append(cleaned_text)
                    count += 1
                    
                if count >= MAX_COMMENTS:
                    break
            
            # --- G·ªòP T·∫§T C·∫¢ TH√ÄNH 1 D√íNG DUY NH·∫§T ---
            # N·ªëi c√°c comment b·∫±ng d·∫•u ch·∫•m "." ho·∫∑c kho·∫£ng tr·∫Øng " "
            merged_text = " . ".join(raw_comments)
            return merged_text
            print(f"‚úÖ ƒê√£ g·ªôp {count} comment.")
            
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i comment: {e}")
            return 'nomal'
    else:
        print("‚ùå Kh√¥ng t√¨m th·∫•y video.")
        return 'nomal'
        
    # Ngh·ªâ nh·∫π ƒë·ªÉ tr√°nh ch·∫∑n IP
    time.sleep(2)


# 3. M·ªü file Output ·ªü ch·∫ø ƒë·ªô 'a' (Append - Ghi n·ªëi ti·∫øp)
# buffer=1: Ghi xu·ªëng ·ªï c·ª©ng ngay l·∫≠p t·ª©c sau m·ªói d√≤ng (tr√°nh m·∫•t ƒëi·ªán m·∫•t d·ªØ li·ªáu)
with open(OUTPUT_FILE, mode='a', newline='', encoding='utf-8', buffering=1) as f:
    
    writer = csv.writer(f)
    
    # N·∫øu file m·ªõi tinh, ghi Header tr∆∞·ªõc
    if not file_exists:
        writer.writerow(['track_artist','track_name','track_popularity','playlist_genre','energy','valence','acousticness','instrumentalness','speechiness','lyrics','comments'])

    # 4. V√≤ng l·∫∑p ch√≠nh (B·∫Øt ƒë·∫ßu t·ª´ start_index)
    # df.iloc[start_index:] gi√∫p c·∫Øt b·ªè ph·∫ßn ƒë√£ ch·∫°y r·ªìi
    for index, row in df.iloc[start_index:].iterrows():
        
        track_name = row['track_name']
        track_artist = row['track_artist']
        track_popularity = row['track_popularity']
        playlist_genre = row['playlist_genre']
        energy = row['energy']
        valence = row['valence']
        acousticness = row['acousticness']
        instrumentalness = row['instrumentalness']
        speechiness = row['speechiness']
        lyrics = row['lyrics']
        term = f"{track_name} {track_artist}"
        
        try:
            # T√¨m l·ªùi
            comments = process_comments(track_name,track_artist)
            
            # GHI NGAY L·∫¨P T·ª®C XU·ªêNG FILE
            writer.writerow([track_name,track_artist,track_popularity,playlist_genre,energy,valence,acousticness,instrumentalness,speechiness,lyrics,comments])
            
            # In ti·∫øn ƒë·ªô cho ƒë·ª° s·ªët ru·ªôt
            # (index + 1) v√¨ index b·∫Øt ƒë·∫ßu t·ª´ 0
            print(f"[{index+1}/{total_songs}] ‚úÖ Xong: {term}")
            
        except Exception as e:
            print(f"‚ùå L·ªói: {term}")
            writer.writerow([track_name,track_artist,track_popularity,playlist_genre,energy,valence,acousticness,instrumentalness,speechiness,lyrics, "normal"])
        
        # Ng·ªß nh·∫π 0.5s ƒë·ªÉ server kh√¥ng ch·∫∑n (quan tr·ªçng v·ªõi 1 tri·ªáu request)
        # N·∫øu m·∫°ng kh·ªèe c√≥ th·ªÉ gi·∫£m xu·ªëng 0.1
        time.sleep(0.3)

print("üéâ ƒê√£ ho√†n th√†nh to√†n b·ªô!")